\section{Classification}
Given a space $X$ and a probability distribution $\tau$ on $X\times \Real$
\footnote{another way to put this would be that for every $x\in X$ there exists a probability distribution $\tau _x$ specifying how likely this $x$ goes together with some real number $r$},
a \textit{regression problem} is the problem of finding a function $f:X\rightarrow\Real$, $f(x)=\mathrm{E}_{\tau _x}$, where  $\mathrm{E}_{\tau _x}$ is the expected value of a random variable $Y$ which has the distribution $\tau _x$ (the marginal distribution of $x$ on $\Real$).

In real-world applications we have a finite set of samples $(x_1,y_1), \dots, (x_n,y_n)$ from which we reconstruct a function that should
\begin{enumerate}
    \item describe the given examples well
    \item make the probability of making large errors for new examples small
\end{enumerate}
This problem is closely related to the idea of interpolation and as we already know fulfilling the first requirement perfectly hardly ever goes hand in hand with the second one. In practice one wants to balance the complexity \footnote{Depending on the concrete implementation there are different definitions of complexity. In the theory of splines (or kernel methods in machine learning) $||f||_\mathbb{H}$, where $\mathbb{H}$ is a reproducing kernel Hilbert space, may be seen as one; there also exist combinatorial approaches and in this work we will introduce a smoothness functional to be minimized.} of a function and its ability to describe given data.

\textit{Classification} may be seen as a special case of regression where we aim to find a function $f:X\rightarrow \{c_1,\dots,c_n\}$.

\section{An Implicit Definition of the Classifier}
\subsection{The Discrete Case}
To develop a better understanding the discrete case will be dealt with in the first place.

Given a set of points $\{x_i\}_{i=1,\dots,n}$, $x_i \in \Real^l$ a weighted graph $G$ can be constructed. For determining which nodes are \textit{adjacent} we will resort to either of two major common definitions:
\begin{enumerate}
    \item \textbf{k nearest neighbours}: find the $k$ nearest nodes
    \item \textbf{$\epsilon$-neighbourhoods}: two nodes $x,y$ are connected if and only if $||x-y||<\epsilon$
\end{enumerate}

Nearby nodes $x_i,x_j$ are connected with weights $W_{ij}=e^{-\frac{||x_i - x_j||^2}{t}}$, the weight between unconnected nodes is set to $0$.\todo{depending on the length of the work either remark that an explanation will be given later on (heat diffusion resembles this (=Gaussian heat kernel) or not at all (in this case set weights to 1 (which may also be seen as $t\rightarrow\infty$, all mathematical properties should stay the same))}

Consider the problem of mapping the weighted graph to the real line so that connected points stay together as close as possible.\footnote{For simplicity's sake assume that the graph is connected.}
As the above defined weights of adjacent nodes are defined in such a way that they are indirectly proportional to their (euclidian) distance it seems natural that $(y_1,\dots,y_n)^T$ is a good map if
\begin{equation} \label{eq:discreteDirichletEnergy}
    \sum_{ij} (y_i - y_j)^2 W_{ij}
\end{equation}
is minimal.

Let $D$ be the degree matrix of the graph $G$, then its Laplacian matrix $\Lagr$ is defined as
\begin{definition}
    $\Lagr$:=D-W
\end{definition}

and \eqref{eq:discreteDirichletEnergy} may be written as 

\begin{equation*}\begin{array}{l l}
\sum_{ij} (y_i - y_j)^2 W_{ij} &= \sum_{ij} (y_i^2 + y_j^2 - 2y_i y_j) W_{ij}\\
    &= \sum_{i} y_i^2 D_{ii} + \sum_{j} y_j^2 D_{jj} - 2\sum_{ij} y_i y_j W_{ij}\\
    &= 2y^T\Lagr y
\end{array}\end{equation*}

Not only do we see that the minimization problem reduces to finding

\begin{equation*}
    \text{argmin}_y y^T\Lagr y
\end{equation*}
but also that $\Lagr$ is positive semidefinite.

However, an additional constraint ($y^T\Lagr\mathbf{1}=0$, where $\mathbf{1}$ is the constant function mapping all nodes to $1$; it is easy to see that this is an eigenfunction to the eigenvalue $0$) is needed as the trivial solution is not excluded.

Additionaly, we want to assign some kind of importance to each of the nodes, based on how many adjacent nodes it has; we do this by adding the additional constraint $y^TDy=1$ which, due to the nature of the objective function \eqref{eq:discreteDirichletEnergy}, leads to the generalized eigenvalue problem $\Lagr y=\lambda Dy$.

\todo{explain why eigenvalues are important here (could resemble the proof why the HÃ¶lder norm for $p=2$ induces the spectral norm(?))}

\subsection{The Continuous Case}
\begin{definition}
    $\Lagr f:=-\mathrm{div}\nabla f$
    \todo{define only for $\Real^n$ or more generally?}
\end{definition}

In analogy to the discrete case let $f:\Man\rightarrow\Real$ be a function that maps points with "small" geodesic distance "close" together on the real line and assume $f\in C^2$.